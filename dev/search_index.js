var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"","category":"section"},{"location":"guide/#Performance-Tips","page":"User Guide","title":"Performance Tips","text":"","category":"section"},{"location":"guide/#Nyström-PCG-Parameters","page":"User Guide","title":"Nyström PCG Parameters","text":"","category":"section"},{"location":"api/#API-Reference","page":"API reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API reference","title":"API reference","text":"","category":"page"},{"location":"api/","page":"API reference","title":"API reference","text":"Modules = [GeNIOS]","category":"page"},{"location":"method/#Algorithm","page":"Solution method","title":"Algorithm","text":"","category":"section"},{"location":"method/#ADMM-iteration","page":"Solution method","title":"ADMM iteration","text":"","category":"section"},{"location":"method/#Fast-linear-system-solves-with-Nyström-PCG","page":"Solution method","title":"Fast linear system solves with Nyström PCG","text":"","category":"section"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"The source files for all examples can be found in /examples.","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"EditURL = \"https://github.com/tjdiamandis/GeNIOS.jl/blob/main/examples/constrained-ls.jl\"","category":"page"},{"location":"examples/constrained-ls/#Constrained-Least-Squares","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"","category":"section"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"This example setsup a constrained least squares problem using the quadratic program interface of GeNIOS.jl. It is from the OSQP docs.","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"Specifically, we want to solve the problem","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"beginarrayll\ntextminimize      Ax - b_2^2 \ntextsubject to    0 leq x leq 1\nendarray","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"using GeNIOS\nusing Random, LinearAlgebra, SparseArrays","category":"page"},{"location":"examples/constrained-ls/#Generating-the-problem-data","page":"Constrained Least Squares","title":"Generating the problem data","text":"","category":"section"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"Random.seed!(1)\nm, n = 30, 20\nAd = sprandn(m, n, 0.7)\nb = randn(m);\nnothing #hide","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"For convenience, we will introdudce a new variable y = Ax - b. The problem becomes","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"beginarrayll\ntextminimize      y^Ty \ntextsubject to    Ax - y = b \n                     0 leq x leq 1\nendarray","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"In OSQP form, this problem is","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"beginarrayll\ntextminimize      y^Ty \ntextsubject to    Ax - y = z_1 \n                     x = z_2 \n                     b leq z_1 leq b \n                     0 leq z_2 leq 1\nendarray","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"or more explicitly","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"beginarrayll\ntextminimize      beginbmatrix x  y endbmatrix^T beginbmatrix 0     I endbmatrix beginbmatrix x  y endbmatrix \ntextsubject to    beginbmatrix A  -I  I  0 endbmatrix beginbmatrix x  y endbmatrix = z\n                     beginbmatrix b  0 endbmatrix leq z leq beginbmatrix b  1 endbmatrix\nendarray","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"P = blockdiag(spzeros(n, n), sparse(I, m, m))\nq = spzeros(n + m)\nM = [\n    Ad                  -sparse(I, m, m);\n    sparse(I, n, n)     spzeros(n, m)\n]\nl = [b; spzeros(n)]\nu = [b; ones(n)];\nnothing #hide","category":"page"},{"location":"examples/constrained-ls/#Building-and-solving-the-problem","page":"Constrained Least Squares","title":"Building and solving the problem","text":"","category":"section"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"Now we setup the solver and solve the problem","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"solver = GeNIOS.QPSolver(P, q, M, l, u)\nres = solve!(\n    solver; options=GeNIOS.SolverOptions(\n        relax=true,\n        max_iters=1000,\n        eps_abs=1e-6,\n        eps_rel=1e-6,\n        verbose=true)\n);\nnothing #hide","category":"page"},{"location":"examples/constrained-ls/#Examining-the-solution","page":"Constrained Least Squares","title":"Examining the solution","text":"","category":"section"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"We can check the solution and its optimality","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"xstar = solver.zk[m+1:end]\nls_residual =  1/√(m) * norm(Ad*xstar - b)\nfeas = all(xstar .>= 0) && all(xstar .<= 1)\nprintln(\"Is feasible? $feas\")\nprintln(\"Least Squres RMSE = $(round(ls_residual, digits=8))\")\nprintln(\"Primal residual: $(round(solver.rp_norm, digits=8))\")\nprintln(\"Dual residual: $(round(solver.rd_norm, digits=8))\")","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"","category":"page"},{"location":"examples/constrained-ls/","page":"Constrained Least Squares","title":"Constrained Least Squares","text":"This page was generated using Literate.jl.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"The source files for all examples can be found in /examples.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"EditURL = \"https://github.com/tjdiamandis/GeNIOS.jl/blob/main/examples/lasso.jl\"","category":"page"},{"location":"examples/lasso/#Lasso","page":"Lasso","title":"Lasso","text":"","category":"section"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"This example sets up a lasso regression problem with three different interfaces provided by GeNIOS.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"Specifically, we want to solve the problem","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"beginarrayll\ntextminimize      (12)Ax - b_2^2 + gamma x_1\nendarray","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"using GeNIOS\nusing Random, LinearAlgebra, SparseArrays","category":"page"},{"location":"examples/lasso/#Generating-the-problem-data","page":"Lasso","title":"Generating the problem data","text":"","category":"section"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"Random.seed!(1)\nm, n = 200, 400\nA = randn(m, n)\nA .-= sum(A, dims=1) ./ m\nnormalize!.(eachcol(A))\nxstar = sprandn(n, 0.1)\nb = A*xstar + 1e-3*randn(m)\nγ = 0.05*norm(A'*b, Inf)","category":"page"},{"location":"examples/lasso/#MLSolver-interface","page":"Lasso","title":"MLSolver interface","text":"","category":"section"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"The easiest interface for this problem is the MLSolver, where we just need to specify f and the regularization parameters","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"f(x) = 0.5*x^2\ndf(x) = x\nd2f(x) = 1.0\nfconj(x) = 0.5*x^2\nλ1 = γ\nλ2 = 0.0\nsolver = GeNIOS.MLSolver(f, df, d2f, λ1, λ2, A, b; fconj=fconj)\nres = solve!(solver; options=GeNIOS.SolverOptions(relax=true, use_dual_gap=true, tol=1e-3, verbose=true))\nrmse = sqrt(1/m*norm(A*solver.zk - b, 2)^2)\nprintln(\"Final RMSE: $(round(rmse, digits=8))\")","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"Note that we also defined the conjugate function of f, defined as","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"f^*(y) = sup_x yx - f(x)","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"which allows us to use the dual gap as a stopping criterion (see appendix C of our paper for a derivation). Specifying the conjugate function is optional, and the solver will fall back to using the primal and dual residuals if it is not specified.","category":"page"},{"location":"examples/lasso/#QPSolver-interface","page":"Lasso","title":"QPSolver interface","text":"","category":"section"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"The QPSolver interface requires us to specify the problem in the form","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"beginarrayll\ntextminimize      (12)x^TPx + q^Tx \ntextsubject to     l leq Mx leq u\nendarray","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"We introduce the new variable t and inntroduce the constraint -t leq x leq t to enforce the ell_1 norm. The problem then becomes","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"beginarrayll\ntextminimize      (12)x^TA^TAx + b^TAx + gamma mathbf1^Tt \ntextsubject to   \nbeginbmatrix0  -inftyendbmatrix\nleq beginbmatrix I  I  I  -I endbmatrix beginbmatrixx  tendbmatrix\nleq beginbmatrixinfty  0endbmatrix\nendarray","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"P = blockdiag(sparse(A'*A), spzeros(n, n))\nq = vcat(-A'*b, γ*ones(n))\nM = [\n    sparse(I, n, n)     sparse(I, n, n);\n    sparse(I, n, n)     -sparse(I, n, n)\n]\nl = [zeros(n); -Inf*ones(n)]\nu = [Inf*ones(n); zeros(n)]\nsolver = GeNIOS.QPSolver(P, q, M, l, u)\nres = solve!(\n    solver; options=GeNIOS.SolverOptions(\n        relax=true,\n        max_iters=1000,\n        eps_abs=1e-4,\n        eps_rel=1e-4,\n        verbose=true)\n);\nrmse = sqrt(1/m*norm(A*solver.xk[1:n] - b, 2)^2)\nprintln(\"Final RMSE: $(round(rmse, digits=8))\")","category":"page"},{"location":"examples/lasso/#Generic-interface","page":"Lasso","title":"Generic interface","text":"","category":"section"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"Finally, we use the generic interface, which provides a large amount of control but is more complicated to use than the specialized interfaces demonstrated above.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"First, we define the custom HessianOperator, which is used to solve the linear system in the x-subproblem. Since the Hessian of the objective is simply A^TA, this operator is simple for the Lasso problem. However, note that, since this is a custom type, we can speed up the multiplication to be more efficient than if we were to use A^TA directly.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"The update! function is called before the solution of the x-subproblem to update the Hessian, if necessary.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"struct HessianLasso{T, S <: AbstractMatrix{T}} <: HessianOperator\n    A::S\n    vm::Vector{T}\nend\nfunction LinearAlgebra.mul!(y, H::HessianLasso, x)\n    mul!(H.vm, H.A, x)\n    mul!(y, H.A', H.vm)\n    return nothing\nend\n\nfunction update!(::HessianLasso, ::Solver)\n    return nothing\nend","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"Now, we define f, its gradient, g, and its proximal operator.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"function f(x, A, b, tmp)\n    mul!(tmp, A, x)\n    @. tmp -= b\n    return 0.5 * sum(w->w^2, tmp)\nend\nf(x) = f(x, A, b, zeros(m))\nfunction grad_f!(g, x, A, b, tmp)\n    mul!(tmp, A, x)\n    @. tmp -= b\n    mul!(g, A', tmp)\n    return nothing\nend\ngrad_f!(g, x) = grad_f!(g, x, A, b, zeros(m))\nHf = HessianLasso(A, zeros(m))\ng(z, γ) = γ*sum(x->abs(x), z)\ng(z) = g(z, γ)\nfunction prox_g!(v, z, ρ)\n    @inline soft_threshold(x::T, κ::T) where {T <: Real} = sign(x) * max(zero(T), abs(x) - κ)\n    v .= soft_threshold.(z, γ/ρ)\nend","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"Finally, we can solve the problem.","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"solver = GeNIOS.GenericSolver(\n    f, grad_f!, Hf,         # f(x)\n    g, prox_g!,             # g(z)\n    -I, zeros(n);           # M, c: Mx + z = c\n    ρ=1.0, α=1.0\n)\nres = solve!(solver; options=GeNIOS.SolverOptions(relax=true, verbose=true))\nrmse = sqrt(1/m*norm(A*solver.zk - b, 2)^2)\nprintln(\"Final RMSE: $(round(rmse, digits=8))\")","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"","category":"page"},{"location":"examples/lasso/","page":"Lasso","title":"Lasso","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = GeNIOS","category":"page"},{"location":"#GeNIOS.jl","page":"Home","title":"GeNIOS.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TODO","category":"page"},{"location":"#Documentation-Contents:","page":"Home","title":"Documentation Contents:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"index.md\", \"method.md\", \"guide.md\"]\nDepth = 1","category":"page"},{"location":"#Examples:","page":"Home","title":"Examples:","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"examples/constrained-ls.md\", \"examples/lasso.md\"]\nDepth = 1","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeNIOS solves convex optimization problems of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginarrayll\ntextminimize      f(x) + g(z) \ntextsubject to    Mx + z = c\nendarray","category":"page"},{"location":"","page":"Home","title":"Home","text":"where x in mathbbR^n and z in mathbbR^m are the optimization variables. The function f is assumed to be smooth, with known first and second derivatives, and the function g must have a known proximal operator.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Compared to conic form programs, the form we use in GeNIOS facilitates custom  subroutines that often provide significant speedups. To ameliorate the extra  complexity, we provide a few interfaces that take advantage of special problem structure.","category":"page"},{"location":"#Interfaces","page":"Home","title":"Interfaces","text":"","category":"section"},{"location":"#QP-interface","page":"Home","title":"QP interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Many important problems in machine learning, finance, operations research, and control can be formulated as QPs. Examples include","category":"page"},{"location":"","page":"Home","title":"Home","text":"Lasso regression\nPortfolio optimization\nTrajectory optimization\nModel predictive control\nAnd many more...","category":"page"},{"location":"","page":"Home","title":"Home","text":"GeNIOS accepts QPs of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginarrayll\ntextminimize      (12)x^TPx + q^Tx \ntextsubject to    l leq Mx leq u\nendarray","category":"page"},{"location":"","page":"Home","title":"Home","text":"which can be constructed using","category":"page"},{"location":"","page":"Home","title":"Home","text":"solver = GeNIOS.QPSolver(P, q, M, l, u)","category":"page"},{"location":"#ML-interface","page":"Home","title":"ML interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In machine learning problems, we can take advantage of additional structure. In our MLSolver, we assume the problem is of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginarrayll\ntextminimize      sum_i=1^m f(a_i^Tx - b_i) + (12)lambda_2x_2^2 + lambda_1x_1\nendarray","category":"page"},{"location":"","page":"Home","title":"Home","text":"where f is the per-sample loss function. Let A be a matrix with rows a_i and b be a vector with entries b_i. The MLSolver is constructed via","category":"page"},{"location":"","page":"Home","title":"Home","text":"solver = GeNIOS.MLSolver(f, df, d2f, λ1, λ2, A, b)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where df, and d2f are scalar functions giving the first and second derivative of f respectively.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the future, we may extend this interface to allow constraints on x, but for now, you can use the generic interface to specify constraints in machine learning problems with non-quadratic objective functions.","category":"page"},{"location":"#Generic-interface","page":"Home","title":"Generic interface","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For power users, we expose a fully generic interface for the problem","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginarrayll\ntextminimize      f(x) + g(z) \ntextsubject to    Mx + z = c\nendarray","category":"page"},{"location":"","page":"Home","title":"Home","text":"Users must specify f, nabla f, nabla^2 f (via a HessianOperator),  g, the proximal operator of g, M, and c. We provide full details of these functions in the User Guide. Also checkout the Lasso example to  see a problem written in all three ways.","category":"page"},{"location":"#Algorithm","page":"Home","title":"Algorithm","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"GeNIOS follows a similar approach to OSQP [1], solving the convex optimization problem using ADMM [2]. Note that the problem form of GeNIOS is, however, more  general than conic form solvers. The key algorithmic differences lies in the  x-subproblem of ADMM. Instead of solving this problem exactly, GeNIOS solves a quadratic approximation to this problem. Recent work [3] shows that this  approximation does not harm ADMM's convergence rate. The subproblem is then solved via the iterative conjugate gradient method with a randomized preconditioner [4]. GeNIOS also incorporates several other heuristics from the literature. Please see our paper for additional details.","category":"page"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The JuMP interface is the easiest way to use GeNIOS. A simple Markowitz portfolio example is below.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using JuMP, GeNIOS\n# TODO:","category":"page"},{"location":"","page":"Home","title":"Home","text":"However, the native interfaces can be called directly by specifying the problem data. Using the QPSolver, is it written as","category":"page"},{"location":"","page":"Home","title":"Home","text":"# TODO:","category":"page"},{"location":"","page":"Home","title":"Home","text":"And, finally, using the fully general interface, it is written as","category":"page"},{"location":"","page":"Home","title":"Home","text":"# TODO:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please see the User Guide for a full explanation of the interfaces, keyword  arguments and performance tips. Also check out the examples as well.","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[1]: Stellato, B., Banjac, G., Goulart, P., Bemporad, A., & Boyd, S. (2020). OSQP: An operator splitting solver for quadratic programs. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2]: Boyd, S., Parikh, N., Chu, E., Peleato, B., & Eckstein, J. (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[3]: Frangella, Z., Zhao, S., Diamandis, T., Stellato, B., & Udell, M. (2023). On the (linear) convergence of Generalized Newton Inexact ADMM.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[4]: Frangella, Z., Tropp, J. A., & Udell, M. (2021). Randomized Nyström Preconditioning.","category":"page"}]
}
